{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.背景介绍\n",
    "\n",
    "本实验主要研究 PyTorch 分布式训练框架下的单机多卡（Single Machine Multi-GPU）并行训练方法，重点探索 torch.distributed 和 torch.nn.parallel.DistributedDataParallel (DDP) 的使用。\n",
    "\n",
    "随着深度学习模型规模的不断增长，单张 GPU 的计算能力往往难以满足高效训练的需求，因此，利用多张 GPU 进行数据并行加速训练成为一种常见的优化方案。\n",
    "\n",
    "本实验采用 PyTorch 框架，实现一个基于 Transformer 结构的 NLP 模型，并使用 DDP 进行多 GPU 训练，以验证其性能和正确性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.实验目的\n",
    "掌握 PyTorch 分布式训练框架的基本概念，包括 torch.distributed、DistributedDataParallel 等核心 API。\n",
    "\n",
    "实现基于 DDP 的数据并行训练，在单机多 GPU 设备上进行深度学习模型训练，并理解其工作原理。\n",
    "\n",
    "理解 DistributedSampler 在多 GPU 训练中的作用，确保数据在多个进程间均匀分配。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.硬件要求\n",
    "\n",
    "2张 GPU（4090、V100、A100等）或 CPU 。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.技术原理\n",
    "\n",
    "本实验的核心是 PyTorch 的 DistributedDataParallel (DDP)，它用于在多个 GPU  设备之间高效地并行化训练过程。其基本原理如下：\n",
    "\n",
    "### 数据并行（Data Parallelism）\n",
    "\n",
    "- 采用 单机多 GPU 数据并行策略，每张 GPU 运行一个独立的训练进程（采用 torch.multiprocessing 进行进程管理）。\n",
    "\n",
    "- 训练数据集在多个 GPU 之间划分，每张 GPU 仅处理部分数据，并行计算梯度。\n",
    "\n",
    "- DistributedSampler 确保每张 GPU 处理的数据不会重复，从而保证训练效率和正确性。\n",
    "\n",
    "### DDP 训练流程\n",
    "\n",
    "每个 GPU 进程独立计算梯度。\n",
    "\n",
    "PyTorch DDP 通过 梯度同步（All-Reduce） 机制，在后向传播时将所有 GPU 的梯度聚合并同步，以确保模型参数在所有 GPU 上保持一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.实验流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 环境配置\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. transformer模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.distributed as dist\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import DistributedSampler\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.out_linear = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        q = self.q_linear(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        k = self.k_linear(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        v = self.v_linear(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-1e9'))\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attn_weights, v)\n",
    "        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n",
    "        return self.out_linear(output)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc2(F.relu(self.fc1(x)))\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = MultiHeadSelfAttention(d_model, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.ffn = FeedForward(d_model, d_ff)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        attn_out = self.attention(x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_out))\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.norm2(x + self.dropout(ffn_out))\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=512, num_heads=8, d_ff=2048, num_layers=6, max_len=5000):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
    "        self.layers = nn.ModuleList([TransformerBlock(d_model, num_heads, d_ff) for _ in range(num_layers)])\n",
    "        self.out_linear = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoding(x)\n",
    "        mask = torch.triu(torch.ones(x.shape[1], x.shape[1]), diagonal=1)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.out_linear(x)\n",
    "\n",
    "model = Transformer(1000, 32, 8, 64, 2, 5000)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.数据集定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLPDataset(Dataset):\n",
    "    def __init__(self, size, length):\n",
    "        self.data = []\n",
    "        for i in range(size):\n",
    "            self.data.append(torch.full((length, ), i))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "dataset = NLPDataset(12, 10)\n",
    "for data in dataset:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3训练核心代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(rank, world_size):\n",
    "    VOCAB_SIZE = 100\n",
    "    D_MODEL = 12\n",
    "    NUM_HEADS = 4\n",
    "    D_FF = 24\n",
    "    NUM_LAYERS = 2\n",
    "    MAX_LEN = 100\n",
    "\n",
    "    DATASET_SIZE = 12\n",
    "    DATASET_LENGTH = 10\n",
    "    BATCH_SIZE = 2\n",
    "\n",
    "    dataset = NLPDataset(size=DATASET_SIZE, length=DATASET_LENGTH)\n",
    "    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank)\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=sampler)\n",
    "\n",
    "    backend = \"nccl\" if torch.cuda.is_available() else \"gloo\"\n",
    "    device = torch.device(f\"cuda:{rank}\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    dist.init_process_group(backend=backend, rank=rank, world_size=world_size)\n",
    "\n",
    "    model = Transformer(\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        d_model=D_MODEL,\n",
    "        num_heads=NUM_HEADS,\n",
    "        d_ff=D_FF,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        max_len=MAX_LEN,\n",
    "    ).to(device)\n",
    "    model = DDP(model, device_ids=[rank] if torch.cuda.is_available() else None)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "    for epoch in range(200):\n",
    "        for batch, data in enumerate(dataloader):\n",
    "            label = data[:, 1:].to(device)\n",
    "            data = data[:, :-1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs.view(-1, VOCAB_SIZE), label.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if rank == 0:\n",
    "                print(f\"Epoch {epoch}, Batch {batch}, Loss: {loss.item()}\")\n",
    "\n",
    "    dist.destroy_process_group()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4启动训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash run.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
