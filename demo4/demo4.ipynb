{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**因 Jupter 对多进程支持不完善，因此分布式实验的代码运行均需要通过脚本执行！！！！！！**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.背景介绍\n",
    "\n",
    "本实验主要研究 Megatron-LM 分布式训练框架下的张量并行训练方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.实验目的\n",
    "了解张量并行的基本思想，利用 Megatron-LM 并行训练框架，完成两卡张量并行训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.硬件要求\n",
    "\n",
    "两张 GPU（4090、V100、A100等）。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.技术原理\n",
    "\n",
    "### 张量并行\n",
    "\n",
    "- 为了解决朴素数据并行显存节省效率不高，朴素流水线并行计算效率不高的问题，张量并行对模型纵向切分，每张 GPU 保存一层的部分权重，在运算时，输入张量在每一张 GPU 上分别计算，然后通过 GPU 之间的通信聚合起来。\n",
    "\n",
    "- 相较于朴素的流水线并行，张量并行每一次计算均有多张 GPU 参与。\n",
    "\n",
    "- 相较于朴素的数据并行，张量并行每一张 GPU 保存部分模型权重。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.实验流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 环境配置\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/NVIDIA/Megatron-LM.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch einops pybind11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. 包导入以及通信组设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"./Megatron-LM\")\n",
    "\n",
    "\n",
    "from megatron.core import parallel_state\n",
    "from megatron.core import dist_checkpointing\n",
    "from megatron.core.pipeline_parallel.schedules import get_forward_backward_func\n",
    "from megatron.core.tensor_parallel.random import model_parallel_cuda_manual_seed\n",
    "from megatron.core.transformer.transformer_config import TransformerConfig\n",
    "from megatron.core.models.gpt.gpt_model import GPTModel\n",
    "from megatron.core.models.gpt.gpt_layer_specs import get_gpt_layer_local_spec\n",
    "from megatron.core.datasets.utils import compile_helpers \n",
    "from megatron.core.datasets.blended_megatron_dataset_builder import BlendedMegatronDatasetBuilder\n",
    "from megatron.core.datasets.gpt_dataset import GPTDatasetConfig, MockGPTDataset\n",
    "from megatron.training.tokenizer.tokenizer import _NullTokenizer\n",
    "\n",
    "\n",
    "_SEQUENCE_LENGTH = 64\n",
    "\n",
    "\n",
    "def initialize_distributed(tensor_model_parallel_size=1, pipeline_model_parallel_size=1):\n",
    "    parallel_state.destroy_model_parallel()\n",
    "\n",
    "    # Torch setup for distributed training\n",
    "    rank = int(os.environ['LOCAL_RANK'])\n",
    "    world_size = torch.cuda.device_count()\n",
    "    torch.cuda.set_device(rank)\n",
    "    torch.distributed.init_process_group(world_size=world_size, rank=rank)\n",
    "\n",
    "    # Megatron core distributed training initialization\n",
    "    parallel_state.initialize_model_parallel(tensor_model_parallel_size, pipeline_model_parallel_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. transformer模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_provider():\n",
    "    \"\"\"Build the model.\"\"\"\n",
    "\n",
    "    transformer_config = TransformerConfig(\n",
    "        num_layers=10, \n",
    "        hidden_size=512, \n",
    "        num_attention_heads=4, \n",
    "        use_cpu_initialization=True, \n",
    "        pipeline_dtype=torch.float32,\n",
    "    )\n",
    "\n",
    "    gpt_model = GPTModel(\n",
    "        config=transformer_config, \n",
    "        transformer_layer_spec=get_gpt_layer_local_spec(), \n",
    "        vocab_size=100, \n",
    "        max_sequence_length=_SEQUENCE_LENGTH,\n",
    "    )\n",
    "\n",
    "    return gpt_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.数据集定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data_iterator():\n",
    "    # 编译辅助函数\n",
    "    if torch.distributed.is_available() and torch.distributed.is_initialized():\n",
    "        if torch.distributed.get_rank() == 0:\n",
    "            compile_helpers()\n",
    "        torch.distributed.barrier()\n",
    "    else:\n",
    "        compile_helpers()\n",
    "\n",
    "    # 配置数据集构建参数\n",
    "    config = GPTDatasetConfig(\n",
    "        random_seed=0,                       # 固定随机种子以保证数据一致性\n",
    "        sequence_length=_SEQUENCE_LENGTH,   # 序列长度\n",
    "        reset_position_ids=False,           # 不重置 position_ids\n",
    "        reset_attention_mask=False,         # 不重置 attention_mask\n",
    "        eod_mask_loss=False,                # 不在 EOD（end of document）位置掩码 loss\n",
    "        tokenizer=_NullTokenizer(vocab_size=_SEQUENCE_LENGTH),  # 使用一个空的 tokenizer（仅用于 mock 数据）\n",
    "    )\n",
    "\n",
    "    datasets = BlendedMegatronDatasetBuilder(\n",
    "        MockGPTDataset,             # 数据集类型\n",
    "        [1000, None, None],         # 数据样本数量配比（这里只使用一个有效子数据集）\n",
    "        lambda: True,               # 启用数据集构建的条件（始终为 True）\n",
    "        config                      # 上面定义的数据集配置\n",
    "    ).build()\n",
    "\n",
    "    train_dataloader = DataLoader(datasets[0], batch_size=128, shuffle=True)\n",
    "\n",
    "    train_iterator = iter(train_dataloader)\n",
    "\n",
    "    return train_iterator  # 返回训练数据迭代器\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.训练核心代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义 forward step 函数，用于执行一次前向传播\n",
    "def forward_step_func(data_iterator, model):\n",
    "\n",
    "    # 定义损失函数\n",
    "    def loss_func(loss_mask: torch.Tensor, output_tensor: torch.Tensor):\n",
    "        losses = output_tensor.float()\n",
    "        loss_mask = loss_mask.view(-1).float()\n",
    "        # 只对有 label 的位置计算损失\n",
    "        loss = torch.sum(losses.view(-1) * loss_mask) / loss_mask.sum()\n",
    "\n",
    "        return loss, {'lm loss': loss}\n",
    "\n",
    "    # 从数据迭代器中取出一批数据，并放到指定设备上\n",
    "    data = next(data_iterator)\n",
    "    tokens = data['tokens'].to(device)\n",
    "    attention_mask = data['attention_mask'].to(device)\n",
    "    position_ids = data['position_ids'].to(device)\n",
    "    labels = data['labels'].to(device)\n",
    "    loss_mask = data['loss_mask'].to(device)\n",
    "\n",
    "    # 调用模型进行前向传播，返回输出张量\n",
    "    output_tensor = model(tokens, position_ids, attention_mask, labels=labels)\n",
    "\n",
    "    return output_tensor, partial(loss_func, loss_mask)\n",
    "\n",
    "\n",
    "# 保存分布式模型 checkpoint\n",
    "def save_distributed_checkpoint(checkpoint_path, gpt_model):\n",
    "    sharded_state_dict = gpt_model.sharded_state_dict(prefix='')\n",
    "    dist_checkpointing.save(sharded_state_dict=sharded_state_dict, checkpoint_dir=checkpoint_path)\n",
    "\n",
    "\n",
    "# 加载分布式模型 checkpoint\n",
    "def load_distributed_checkpoint(checkpoint_path, gpt_model):\n",
    "    sharded_state_dict = gpt_model.sharded_state_dict(prefix='')\n",
    "    checkpoint = dist_checkpointing.load(sharded_state_dict=sharded_state_dict, checkpoint_dir=checkpoint_path)\n",
    "    gpt_model.load_state_dict(checkpoint)\n",
    "    return gpt_model\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 初始化分布式训练，设置张量并行大小为 2，流水线并行为 1（即不使用流水线并行）\n",
    "    initialize_distributed(tensor_model_parallel_size=2, pipeline_model_parallel_size=1)\n",
    "\n",
    "    # 设置模型并行随机种子，保证不同设备上初始化一致\n",
    "    model_parallel_cuda_manual_seed(123)\n",
    "\n",
    "    # 初始化模型\n",
    "    gpt_model = model_provider()\n",
    "\n",
    "    device = torch.device(\"cuda\")\n",
    "    gpt_model.to(device)\n",
    "\n",
    "    optim = Adam(gpt_model.parameters())\n",
    "\n",
    "    train_iterator = get_train_data_iterator()\n",
    "\n",
    "    forward_backward_func = get_forward_backward_func()\n",
    "\n",
    "    # 训练 200 次迭代\n",
    "    for _ in range(200):\n",
    "        optim.zero_grad()\n",
    "\n",
    "        losses_reduced = forward_backward_func(\n",
    "            forward_step_func=forward_step_func,  # 前向函数\n",
    "            data_iterator=train_iterator,        # 数据迭代器\n",
    "            model=gpt_model,                     # 模型\n",
    "            num_microbatches=1,                  # microbatch 数量\n",
    "            seq_length=_SEQUENCE_LENGTH,         # 输入序列长度\n",
    "            micro_batch_size=32,                 # 每个 microbatch 的 batch size\n",
    "            decoder_seq_length=_SEQUENCE_LENGTH, # decoder 长度（通常等于输入）\n",
    "            forward_only=False,                  # 训练模式（非推理）\n",
    "        )\n",
    "\n",
    "        optim.step()\n",
    "\n",
    "        print(f\"Losses reduced :  {losses_reduced}\")\n",
    "\n",
    "    # 保存模型 checkpoint 到 ckpt 目录\n",
    "    ckpt_path = os.getcwd() + \"/ckpt\"\n",
    "    Path(ckpt_path).mkdir(exist_ok=True)\n",
    "    save_distributed_checkpoint(gpt_model=gpt_model, checkpoint_path=ckpt_path)\n",
    "\n",
    "    # 加载保存的模型\n",
    "    gpt_model = load_distributed_checkpoint(\n",
    "        gpt_model=gpt_model, checkpoint_path=ckpt_path\n",
    "    )\n",
    "    gpt_model.to(device)\n",
    "    print(\"Successfully loaded the model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd\n",
    "!CUDA_VISIBLE_DEVICES=6,7 torchrun --nproc-per-node 2 train.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2.5.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
