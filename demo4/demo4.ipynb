{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.背景介绍\n",
    "\n",
    "本实验主要研究 Megatron-LM 分布式训练框架下的张量并行训练方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.实验目的\n",
    "了解张量并行的基本思想，利用 Megatron-LM 并行训练框架，完成两卡张量并行训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.硬件要求\n",
    "\n",
    "两张 GPU（4090、V100、A100等）。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.技术原理\n",
    "\n",
    "### 张量并行\n",
    "\n",
    "为了解决朴素数据并行显存节省效率不高，朴素流水线并行计算效率不高的问题，张量并行对模型纵向切分，每张 GPU 保存一层的部分权重，在运算时，输入张量在每一张 GPU 上分别计算，然后通过 GPU 之间的通信聚合起来。\n",
    "\n",
    "相较于朴素的流水线并行，张量并行每一次计算均有多张 GPU 参与。\n",
    "\n",
    "相较于朴素的数据并行，张量并行每一张 GPU 保存部分模型权重。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.实验流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 环境配置\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Megatron-LM'...\n",
      "remote: Enumerating objects: 48313, done.\u001b[K\n",
      "remote: Counting objects: 100% (249/249), done.\u001b[K\n",
      "remote: Compressing objects: 100% (121/121), done.\u001b[K\n",
      "remote: Total 48313 (delta 162), reused 155 (delta 128), pack-reused 48064 (from 2)\u001b[K\n",
      "Receiving objects: 100% (48313/48313), 18.04 MiB | 306.00 KiB/s, done.\n",
      "Resolving deltas: 100% (33793/33793), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/NVIDIA/Megatron-LM.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: einops in /nvme/zhanglantian/anaconda3/envs/torch2.5.1/lib/python3.10/site-packages (0.8.1)\n",
      "Collecting pybind11\n",
      "  Using cached pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
      "Using cached pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
      "Installing collected packages: pybind11\n",
      "Successfully installed pybind11-2.13.6\n"
     ]
    }
   ],
   "source": [
    "!pip install torch einops pybind11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. 包导入以及通信组设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nvme/zhanglantian/project/DistributedTrainingDemos/demo4/./Megatron-LM/megatron/core/transformer/multi_token_prediction.py:60: UserWarning: Apex is not installed. Falling back to Torch Norm\n",
      "  warnings.warn('Apex is not installed. Falling back to Torch Norm')\n",
      "/nvme/zhanglantian/project/DistributedTrainingDemos/demo4/./Megatron-LM/megatron/core/models/gpt/gpt_layer_specs.py:64: UserWarning: Apex is not installed. Falling back to Torch Norm\n",
      "  warnings.warn('Apex is not installed. Falling back to Torch Norm')\n",
      "/nvme/zhanglantian/project/DistributedTrainingDemos/demo4/./Megatron-LM/megatron/core/models/retro/encoder_spec.py:47: UserWarning: Apex is not installed. Falling back to Torch Norm\n",
      "  warnings.warn(f'Apex is not installed. Falling back to Torch Norm')\n",
      "/nvme/zhanglantian/project/DistributedTrainingDemos/demo4/./Megatron-LM/megatron/core/models/retro/decoder_spec.py:39: UserWarning: Apex is not installed. Falling back to Torch Norm\n",
      "  warnings.warn(f'Apex is not installed. Falling back to Torch Norm')\n",
      "/nvme/zhanglantian/project/DistributedTrainingDemos/demo4/./Megatron-LM/megatron/training/utils.py:20: UserWarning: Transformer Engine and Apex are not installed. Falling back to local implementations of multi_tensor_applier and multi_tensor_l2norm\n",
      "  warnings.warn(\n",
      "/nvme/zhanglantian/project/DistributedTrainingDemos/demo4/./Megatron-LM/megatron/core/optimizer/__init__.py:18: UserWarning: Transformer Engine and Apex are not installed. Falling back to Torch optimizers.\n",
      "  warnings.warn(\n",
      "/nvme/zhanglantian/project/DistributedTrainingDemos/demo4/./Megatron-LM/megatron/core/optimizer/optimizer.py:28: UserWarning: Transformer Engine and Apex are not installed. Falling back to local implementations of multi_tensor_applier and multi_tensor_scale\n",
      "  warnings.warn(\n",
      "/nvme/zhanglantian/project/DistributedTrainingDemos/demo4/./Megatron-LM/megatron/core/optimizer/clip_grads.py:29: UserWarning: Transformer Engine and Apex are not installed. Falling back to local implementations of multi_tensor_applier, multi_tensor_l2norm, and multi_tensor_scale\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"./Megatron-LM\")\n",
    "\n",
    "\n",
    "from megatron.core import parallel_state\n",
    "from megatron.core import dist_checkpointing\n",
    "from megatron.core.pipeline_parallel.schedules import get_forward_backward_func\n",
    "from megatron.core.tensor_parallel.random import model_parallel_cuda_manual_seed\n",
    "from megatron.core.transformer.transformer_config import TransformerConfig\n",
    "from megatron.core.models.gpt.gpt_model import GPTModel\n",
    "from megatron.core.models.gpt.gpt_layer_specs import get_gpt_layer_local_spec\n",
    "from megatron.core.datasets.utils import compile_helpers \n",
    "from megatron.core.datasets.blended_megatron_dataset_builder import BlendedMegatronDatasetBuilder\n",
    "from megatron.core.datasets.gpt_dataset import GPTDatasetConfig, MockGPTDataset\n",
    "from megatron.training.tokenizer.tokenizer import _NullTokenizer\n",
    "\n",
    "\n",
    "_SEQUENCE_LENGTH = 64\n",
    "\n",
    "\n",
    "def initialize_distributed(tensor_model_parallel_size=1, pipeline_model_parallel_size=1):\n",
    "    parallel_state.destroy_model_parallel()\n",
    "\n",
    "    # Torch setup for distributed training\n",
    "    rank = int(os.environ['LOCAL_RANK'])\n",
    "    world_size = torch.cuda.device_count()\n",
    "    torch.cuda.set_device(rank)\n",
    "    torch.distributed.init_process_group(world_size=world_size, rank=rank)\n",
    "\n",
    "    # Megatron core distributed training initialization\n",
    "    parallel_state.initialize_model_parallel(tensor_model_parallel_size, pipeline_model_parallel_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. transformer模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_provider():\n",
    "    \"\"\"Build the model.\"\"\"\n",
    "\n",
    "    transformer_config = TransformerConfig(\n",
    "        num_layers=10, \n",
    "        hidden_size=512, \n",
    "        num_attention_heads=4, \n",
    "        use_cpu_initialization=True, \n",
    "        pipeline_dtype=torch.float32,\n",
    "    )\n",
    "\n",
    "    gpt_model = GPTModel(\n",
    "        config=transformer_config, \n",
    "        transformer_layer_spec=get_gpt_layer_local_spec(), \n",
    "        vocab_size=100, \n",
    "        max_sequence_length=_SEQUENCE_LENGTH,\n",
    "    )\n",
    "\n",
    "    return gpt_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.数据集定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data_iterator():\n",
    "    if torch.distributed.is_available() and torch.distributed.is_initialized():\n",
    "        if torch.distributed.get_rank() == 0:\n",
    "            compile_helpers()\n",
    "        torch.distributed.barrier()\n",
    "    else:\n",
    "        compile_helpers()\n",
    "\n",
    "    config = GPTDatasetConfig(\n",
    "        random_seed=0,\n",
    "        sequence_length=_SEQUENCE_LENGTH,\n",
    "        reset_position_ids=False,\n",
    "        reset_attention_mask=False,\n",
    "        eod_mask_loss=False,\n",
    "        tokenizer=_NullTokenizer(vocab_size=_SEQUENCE_LENGTH),\n",
    "    )\n",
    "\n",
    "    datasets = BlendedMegatronDatasetBuilder(\n",
    "        MockGPTDataset, [1000, None, None], lambda: True, config\n",
    "    ).build()\n",
    "\n",
    "    train_dataloader = DataLoader(datasets[0], batch_size=128, shuffle=True)\n",
    "\n",
    "    train_iterator = iter(train_dataloader)\n",
    "\n",
    "    return train_iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.训练核心代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_step_func(data_iterator, model):\n",
    "\n",
    "    def loss_func(loss_mask: torch.Tensor, output_tensor: torch.Tensor):\n",
    "\n",
    "        losses = output_tensor.float()\n",
    "        loss_mask = loss_mask.view(-1).float()\n",
    "        loss = torch.sum(losses.view(-1) * loss_mask) / loss_mask.sum()\n",
    "        # If you have data parallel reduce loss across data parallel groups.\n",
    "        # If pipeline parallel, loss computation is done only in last stage.\n",
    "\n",
    "        return loss, {'lm loss': loss}\n",
    "\n",
    "    data = next(data_iterator)\n",
    "    tokens = data['tokens'].to(device)\n",
    "    attention_mask = data['attention_mask'].to(device)\n",
    "    position_ids = data['position_ids'].to(device)\n",
    "    labels = data['labels'].to(device)\n",
    "    loss_mask = data['loss_mask'].to(device)\n",
    "\n",
    "    output_tensor = model(tokens, position_ids, attention_mask,\n",
    "                          labels=labels)\n",
    "\n",
    "    return output_tensor, partial(loss_func, loss_mask)\n",
    "\n",
    "def save_distributed_checkpoint(checkpoint_path, gpt_model):\n",
    "    sharded_state_dict = gpt_model.sharded_state_dict(prefix='')\n",
    "    dist_checkpointing.save(sharded_state_dict=sharded_state_dict, checkpoint_dir=checkpoint_path)\n",
    "\n",
    "def load_distributed_checkpoint(checkpoint_path, gpt_model):\n",
    "    sharded_state_dict=gpt_model.sharded_state_dict(prefix='')\n",
    "    checkpoint = dist_checkpointing.load(sharded_state_dict=sharded_state_dict, checkpoint_dir=checkpoint_path)\n",
    "    gpt_model.load_state_dict(checkpoint)\n",
    "    return gpt_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nvme/zhanglantian/project/DistributedTrainingDemos/demo4\n",
      "W0325 15:59:32.875000 62447 site-packages/torch/distributed/run.py:793] \n",
      "W0325 15:59:32.875000 62447 site-packages/torch/distributed/run.py:793] *****************************************\n",
      "W0325 15:59:32.875000 62447 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0325 15:59:32.875000 62447 site-packages/torch/distributed/run.py:793] *****************************************\n",
      "/nvme/zhanglantian/project/DistributedTrainingDemos/demo4/./Megatron-LM/megatron/core/transformer/multi_token_prediction.py:60: UserWarning: Apex is not installed. Falling back to Torch Norm\n",
      "  warnings.warn('Apex is not installed. Falling back to Torch Norm')\n",
      "/nvme/zhanglantian/project/DistributedTrainingDemos/demo4/./Megatron-LM/megatron/core/models/gpt/gpt_layer_specs.py:64: UserWarning: Apex is not installed. Falling back to Torch Norm\n",
      "  warnings.warn('Apex is not installed. Falling back to Torch Norm')\n",
      "/nvme/zhanglantian/project/DistributedTrainingDemos/demo4/./Megatron-LM/megatron/core/transformer/multi_token_prediction.py:60: UserWarning: Apex is not installed. Falling back to Torch Norm\n",
      "  warnings.warn('Apex is not installed. Falling back to Torch Norm')\n",
      "/nvme/zhanglantian/project/DistributedTrainingDemos/demo4/./Megatron-LM/megatron/core/models/gpt/gpt_layer_specs.py:64: UserWarning: Apex is not installed. Falling back to Torch Norm\n",
      "  warnings.warn('Apex is not installed. Falling back to Torch Norm')\n",
      "/nvme/zhanglantian/project/DistributedTrainingDemos/demo4/./Megatron-LM/megatron/core/models/retro/encoder_spec.py:47: UserWarning: Apex is not installed. Falling back to Torch Norm\n",
      "  warnings.warn(f'Apex is not installed. Falling back to Torch Norm')\n",
      "/nvme/zhanglantian/project/DistributedTrainingDemos/demo4/./Megatron-LM/megatron/core/models/retro/decoder_spec.py:39: UserWarning: Apex is not installed. Falling back to Torch Norm\n",
      "  warnings.warn(f'Apex is not installed. Falling back to Torch Norm')\n",
      "/nvme/zhanglantian/project/DistributedTrainingDemos/demo4/./Megatron-LM/megatron/training/utils.py:20: UserWarning: Transformer Engine and Apex are not installed. Falling back to local implementations of multi_tensor_applier and multi_tensor_l2norm\n",
      "  warnings.warn(\n",
      "/nvme/zhanglantian/project/DistributedTrainingDemos/demo4/./Megatron-LM/megatron/core/optimizer/__init__.py:18: UserWarning: Transformer Engine and Apex are not installed. Falling back to Torch optimizers.\n",
      "  warnings.warn(\n",
      "/nvme/zhanglantian/project/DistributedTrainingDemos/demo4/./Megatron-LM/megatron/core/optimizer/optimizer.py:28: UserWarning: Transformer Engine and Apex are not installed. Falling back to local implementations of multi_tensor_applier and multi_tensor_scale\n",
      "  warnings.warn(\n",
      "/nvme/zhanglantian/project/DistributedTrainingDemos/demo4/./Megatron-LM/megatron/core/optimizer/clip_grads.py:29: UserWarning: Transformer Engine and Apex are not installed. Falling back to local implementations of multi_tensor_applier, multi_tensor_l2norm, and multi_tensor_scale\n",
      "  warnings.warn(\n",
      "/nvme/zhanglantian/project/DistributedTrainingDemos/demo4/./Megatron-LM/megatron/core/models/retro/encoder_spec.py:47: UserWarning: Apex is not installed. Falling back to Torch Norm\n",
      "  warnings.warn(f'Apex is not installed. Falling back to Torch Norm')\n",
      "/nvme/zhanglantian/project/DistributedTrainingDemos/demo4/./Megatron-LM/megatron/core/models/retro/decoder_spec.py:39: UserWarning: Apex is not installed. Falling back to Torch Norm\n",
      "  warnings.warn(f'Apex is not installed. Falling back to Torch Norm')\n",
      "/nvme/zhanglantian/project/DistributedTrainingDemos/demo4/./Megatron-LM/megatron/training/utils.py:20: UserWarning: Transformer Engine and Apex are not installed. Falling back to local implementations of multi_tensor_applier and multi_tensor_l2norm\n",
      "  warnings.warn(\n",
      "/nvme/zhanglantian/project/DistributedTrainingDemos/demo4/./Megatron-LM/megatron/core/optimizer/__init__.py:18: UserWarning: Transformer Engine and Apex are not installed. Falling back to Torch optimizers.\n",
      "  warnings.warn(\n",
      "/nvme/zhanglantian/project/DistributedTrainingDemos/demo4/./Megatron-LM/megatron/core/optimizer/optimizer.py:28: UserWarning: Transformer Engine and Apex are not installed. Falling back to local implementations of multi_tensor_applier and multi_tensor_scale\n",
      "  warnings.warn(\n",
      "/nvme/zhanglantian/project/DistributedTrainingDemos/demo4/./Megatron-LM/megatron/core/optimizer/clip_grads.py:29: UserWarning: Transformer Engine and Apex are not installed. Falling back to local implementations of multi_tensor_applier, multi_tensor_l2norm, and multi_tensor_scale\n",
      "  warnings.warn(\n",
      "make: Entering directory `/nvme/zhanglantian/project/DistributedTrainingDemos/demo4/Megatron-LM/megatron/core/datasets'\n",
      "make: Nothing to be done for `default'.\n",
      "make: Leaving directory `/nvme/zhanglantian/project/DistributedTrainingDemos/demo4/Megatron-LM/megatron/core/datasets'\n",
      "INFO:megatron.core.datasets.blended_megatron_dataset_config:Let mock = True, as both blend and blend_per_split are None\n",
      "INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split = 1,1,1, an arbitrarily even split, as mock is True\n",
      "INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 0.3333333333333333), (0.3333333333333333, 0.6666666666666666), (0.6666666666666666, 1.0)]\n",
      "INFO:megatron.core.datasets.blended_megatron_dataset_builder:Building MockGPTDataset splits with sizes=[1000, None, None] and config=GPTDatasetConfig(random_seed=0, sequence_length=64, blend=None, blend_per_split=None, split='1,1,1', split_matrix=[(0, 0.3333333333333333), (0.3333333333333333, 0.6666666666666666), (0.6666666666666666, 1.0)], num_dataset_builder_threads=1, path_to_cache=None, mmap_bin_files=True, mock=True, tokenizer=<megatron.training.tokenizer.tokenizer._NullTokenizer object at 0x7f6b54caf010>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=True, drop_last_partial_validation_sequence=True, add_extra_token_to_sequence=True, s3_cache_path=None)\n",
      "INFO:megatron.core.datasets.gpt_dataset:Build and save the MockGPTDataset train indices\n",
      "WARNING:megatron.core.datasets.gpt_dataset:Unable to save MockGPTDataset indexes because path_to_cache is None\n",
      "INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 1065473\n",
      "INFO:megatron.core.datasets.gpt_dataset:> total number of epochs: 1\n",
      "INFO:megatron.core.datasets.gpt_dataset:Build and save the MockGPTDataset valid indices\n",
      "WARNING:megatron.core.datasets.gpt_dataset:Unable to save MockGPTDataset indexes because path_to_cache is None\n",
      "INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 1064995\n",
      "INFO:megatron.core.datasets.gpt_dataset:> total number of epochs: 1\n",
      "INFO:megatron.core.datasets.gpt_dataset:Build and save the MockGPTDataset test indices\n",
      "WARNING:megatron.core.datasets.gpt_dataset:Unable to save MockGPTDataset indexes because path_to_cache is None\n",
      "INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 1066985\n",
      "INFO:megatron.core.datasets.gpt_dataset:> total number of epochs: 1\n",
      "/nvme/zhanglantian/project/DistributedTrainingDemos/demo4/./Megatron-LM/megatron/core/tensor_parallel/layers.py:661: UserWarning: When using async grad allreduce it is recommended to set the environment variable CUDA_DEVICE_MAX_CONNECTIONS to 1 for maximum speedup\n",
      "  warnings.warn(\n",
      "/nvme/zhanglantian/project/DistributedTrainingDemos/demo4/./Megatron-LM/megatron/core/tensor_parallel/layers.py:661: UserWarning: When using async grad allreduce it is recommended to set the environment variable CUDA_DEVICE_MAX_CONNECTIONS to 1 for maximum speedup\n",
      "  warnings.warn(\n",
      "Losses reduced :  [{'lm loss': tensor(3.6390, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.6390, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.4274, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.4274, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.7666, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.7666, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8002, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8002, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8225, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8225, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8229, device='cuda:0', grad_fn=<DivBackward0>)}]Losses reduced :  [{'lm loss': tensor(3.8229, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "\n",
      "Losses reduced :  [{'lm loss': tensor(3.7985, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.7985, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.7815, device='cuda:0', grad_fn=<DivBackward0>)}]Losses reduced :  [{'lm loss': tensor(3.7815, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "\n",
      "Losses reduced :  [{'lm loss': tensor(3.7618, device='cuda:0', grad_fn=<DivBackward0>)}]Losses reduced :  [{'lm loss': tensor(3.7618, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "\n",
      "Losses reduced :  [{'lm loss': tensor(3.7784, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.7784, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.7953, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.7953, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.7869, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.7869, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.7886, device='cuda:1', grad_fn=<DivBackward0>)}]Losses reduced :  [{'lm loss': tensor(3.7886, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "\n",
      "Losses reduced :  [{'lm loss': tensor(3.8021, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8021, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8105, device='cuda:0', grad_fn=<DivBackward0>)}]Losses reduced :  [{'lm loss': tensor(3.8105, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "\n",
      "Losses reduced :  [{'lm loss': tensor(3.8064, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8064, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8152, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8152, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8110, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8110, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8211, device='cuda:0', grad_fn=<DivBackward0>)}]Losses reduced :  [{'lm loss': tensor(3.8211, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "\n",
      "Losses reduced :  [{'lm loss': tensor(3.8232, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8232, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8239, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8239, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8224, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8224, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8194, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8194, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8392, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8392, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8286, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8286, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8163, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8163, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8146, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8146, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8158, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8158, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8274, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8274, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8310, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8310, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8412, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8412, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8353, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8353, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8334, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8334, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8205, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8205, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8788, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8788, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8525, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8525, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8459, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8459, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8674, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8674, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8676, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8676, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8604, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8604, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8367, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8367, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8658, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8658, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8315, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8315, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8450, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8450, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8438, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8438, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8557, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8557, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8375, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8375, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8485, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8485, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8407, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8407, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8221, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8221, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8234, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8234, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8145, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8145, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8410, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8410, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8307, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8307, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8237, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8237, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8233, device='cuda:0', grad_fn=<DivBackward0>)}]Losses reduced :  [{'lm loss': tensor(3.8233, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "\n",
      "Losses reduced :  [{'lm loss': tensor(3.8615, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8615, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8180, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8180, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8491, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8491, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8409, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8409, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8352, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8352, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8349, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8349, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8583, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8583, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8369, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8369, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8259, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8259, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8392, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8392, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8498, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8498, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8487, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8487, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8385, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8385, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8351, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8351, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8491, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8491, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8563, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8563, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8358, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8358, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8515, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8515, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8678, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8678, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8299, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8299, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8482, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8482, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8530, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8530, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8322, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8322, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8353, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8353, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8396, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8396, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8219, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8219, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8422, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8422, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8578, device='cuda:1', grad_fn=<DivBackward0>)}]Losses reduced :  [{'lm loss': tensor(3.8578, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "\n",
      "Losses reduced :  [{'lm loss': tensor(3.8176, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8176, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8327, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8327, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8139, device='cuda:1', grad_fn=<DivBackward0>)}]Losses reduced :  [{'lm loss': tensor(3.8139, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "\n",
      "Losses reduced :  [{'lm loss': tensor(3.8089, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8089, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8108, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8108, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8174, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8174, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8075, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8075, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8251, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8251, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8173, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8173, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8015, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8015, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.7995, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.7995, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8374, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8374, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8053, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8053, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8022, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8022, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8217, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8217, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8166, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8166, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8622, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8622, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8162, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8162, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8320, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8320, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8377, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8377, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8321, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8321, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8327, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8327, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8443, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8443, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8355, device='cuda:1', grad_fn=<DivBackward0>)}]Losses reduced :  [{'lm loss': tensor(3.8355, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "\n",
      "Losses reduced :  [{'lm loss': tensor(3.8378, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8378, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8334, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8334, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8408, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8408, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8343, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8343, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8367, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8367, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8159, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8159, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8517, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8517, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.7980, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.7980, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8376, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8376, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.7873, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.7873, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8198, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8198, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8365, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8365, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.7988, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.7988, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8378, device='cuda:1', grad_fn=<DivBackward0>)}]Losses reduced :  [{'lm loss': tensor(3.8378, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "\n",
      "Losses reduced :  [{'lm loss': tensor(3.8688, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8688, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8087, device='cuda:1', grad_fn=<DivBackward0>)}]Losses reduced :  [{'lm loss': tensor(3.8087, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "\n",
      "Losses reduced :  [{'lm loss': tensor(3.8414, device='cuda:0', grad_fn=<DivBackward0>)}]Losses reduced :  [{'lm loss': tensor(3.8414, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "\n",
      "Losses reduced :  [{'lm loss': tensor(3.8200, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8200, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8495, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8495, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8205, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8205, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8310, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8310, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8454, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8454, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8596, device='cuda:0', grad_fn=<DivBackward0>)}]Losses reduced :  [{'lm loss': tensor(3.8596, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "\n",
      "Losses reduced :  [{'lm loss': tensor(3.8379, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8379, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8301, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8301, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8701, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8701, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8318, device='cuda:0', grad_fn=<DivBackward0>)}]Losses reduced :  [{'lm loss': tensor(3.8318, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "\n",
      "Losses reduced :  [{'lm loss': tensor(3.8106, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8106, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8282, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8282, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8471, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8471, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8404, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8404, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8313, device='cuda:0', grad_fn=<DivBackward0>)}]Losses reduced :  [{'lm loss': tensor(3.8313, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "\n",
      "Losses reduced :  [{'lm loss': tensor(3.8337, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8337, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8306, device='cuda:1', grad_fn=<DivBackward0>)}]Losses reduced :  [{'lm loss': tensor(3.8306, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "\n",
      "Losses reduced :  [{'lm loss': tensor(3.8446, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8446, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8474, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8474, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8109, device='cuda:1', grad_fn=<DivBackward0>)}]Losses reduced :  [{'lm loss': tensor(3.8109, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "\n",
      "Losses reduced :  [{'lm loss': tensor(3.8403, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8403, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8412, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8412, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8360, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8360, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8264, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8264, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8314, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8314, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8749, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8749, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8426, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8426, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8355, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8355, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8254, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8254, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8260, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8260, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8485, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8485, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8428, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8428, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8318, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8318, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8137, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8137, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8001, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8001, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8166, device='cuda:0', grad_fn=<DivBackward0>)}]Losses reduced :  [{'lm loss': tensor(3.8166, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "\n",
      "Losses reduced :  [{'lm loss': tensor(3.8189, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8189, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8294, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8294, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8136, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8136, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8291, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8291, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8275, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8275, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8413, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8413, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8164, device='cuda:1', grad_fn=<DivBackward0>)}]Losses reduced :  [{'lm loss': tensor(3.8164, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "\n",
      "Losses reduced :  [{'lm loss': tensor(3.8398, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8398, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8346, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8346, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8258, device='cuda:0', grad_fn=<DivBackward0>)}]Losses reduced :  [{'lm loss': tensor(3.8258, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "\n",
      "Losses reduced :  [{'lm loss': tensor(3.8314, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8314, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8405, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8405, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8250, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8250, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8397, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8397, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8154, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8154, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8098, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8098, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8460, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8460, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8205, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8205, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8296, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8296, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8240, device='cuda:1', grad_fn=<DivBackward0>)}]Losses reduced :  [{'lm loss': tensor(3.8240, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "\n",
      "Losses reduced :  [{'lm loss': tensor(3.8348, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8348, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8201, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8201, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8161, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8161, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8419, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8419, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8213, device='cuda:0', grad_fn=<DivBackward0>)}]Losses reduced :  [{'lm loss': tensor(3.8213, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "\n",
      "Losses reduced :  [{'lm loss': tensor(3.8216, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8216, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8431, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8431, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8289, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8289, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8576, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8576, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8380, device='cuda:1', grad_fn=<DivBackward0>)}]Losses reduced :  [{'lm loss': tensor(3.8380, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "\n",
      "Losses reduced :  [{'lm loss': tensor(3.8572, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8572, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8401, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8401, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8407, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8407, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8345, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8345, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8313, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8313, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8345, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8345, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8631, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8631, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8251, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8251, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8434, device='cuda:1', grad_fn=<DivBackward0>)}]\n",
      "Losses reduced :  [{'lm loss': tensor(3.8434, device='cuda:0', grad_fn=<DivBackward0>)}]\n",
      "/nvme/zhanglantian/project/DistributedTrainingDemos/demo4/./Megatron-LM/megatron/core/transformer/transformer_layer.py:362: UserWarning: TransformerLayer._get_layer_offset is deprecated.Please use get_transformer_layer_offset instead.\n",
      "  warnings.warn(\n",
      "/nvme/zhanglantian/project/DistributedTrainingDemos/demo4/./Megatron-LM/megatron/core/transformer/transformer_layer.py:362: UserWarning: TransformerLayer._get_layer_offset is deprecated.Please use get_transformer_layer_offset instead.\n",
      "  warnings.warn(\n",
      "WARNING:megatron.core.dist_checkpointing.serialization:Overwriting old incomplete / corrupted checkpoint...\n",
      "/nvme/zhanglantian/anaconda3/envs/torch2.5.1/lib/python3.10/site-packages/torch/distributed/checkpoint/filesystem.py:490: UserWarning: Detected an existing checkpoint in /nvme/zhanglantian/project/DistributedTrainingDemos/demo4/ckpt/.metadata, overwriting since self.overwrite=True. Past version 2.5 of PyTorch, `overwrite` will default to False. Set this variable to True to maintain this functionality or False to raise when an existing checkpoint is found.\n",
      "  warnings.warn(\n",
      "/nvme/zhanglantian/anaconda3/envs/torch2.5.1/lib/python3.10/site-packages/torch/distributed/checkpoint/filesystem.py:490: UserWarning: Detected an existing checkpoint in /nvme/zhanglantian/project/DistributedTrainingDemos/demo4/ckpt/.metadata, overwriting since self.overwrite=True. Past version 2.5 of PyTorch, `overwrite` will default to False. Set this variable to True to maintain this functionality or False to raise when an existing checkpoint is found.\n",
      "  warnings.warn(\n",
      "/nvme/zhanglantian/project/DistributedTrainingDemos/demo4/./Megatron-LM/megatron/core/dist_checkpointing/strategies/torch.py:847: FutureWarning: `load_state_dict` is deprecated and will be removed in future versions. Please use `load` instead.\n",
      "  checkpoint.load_state_dict(\n",
      "/nvme/zhanglantian/project/DistributedTrainingDemos/demo4/./Megatron-LM/megatron/core/dist_checkpointing/strategies/torch.py:847: FutureWarning: `load_state_dict` is deprecated and will be removed in future versions. Please use `load` instead.\n",
      "  checkpoint.load_state_dict(\n",
      "/nvme/zhanglantian/anaconda3/envs/torch2.5.1/lib/python3.10/site-packages/torch/distributed/checkpoint/planner_helpers.py:314: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n",
      "  device = getattr(value, \"device\", None)\n",
      "/nvme/zhanglantian/anaconda3/envs/torch2.5.1/lib/python3.10/site-packages/torch/distributed/checkpoint/planner_helpers.py:314: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n",
      "  device = getattr(value, \"device\", None)\n",
      "Successfully loaded the model\n",
      "Successfully loaded the model\n",
      "[rank1]:[W325 16:00:00.313622937 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n",
      "[rank0]:[W325 16:00:00.323522987 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!CUDA_VISIBLE_DEVICES=6,7 torchrun --nproc-per-node 2 train.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2.5.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
