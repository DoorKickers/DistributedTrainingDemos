{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.背景介绍\n",
    "\n",
    "本实验主要研究 Megatron-LM 分布式训练框架下的张量并行训练方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.实验目的\n",
    "了解张量并行的基本思想，利用 Megatron-LM 并行训练框架，完成两卡张量并行训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.硬件要求\n",
    "\n",
    "两张 GPU（4090、V100、A100等）。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.技术原理\n",
    "\n",
    "### 张量并行\n",
    "\n",
    "为了解决朴素数据并行显存节省效率不高，朴素流水线并行计算效率不高的问题，张量并行对模型纵向切分，每张 GPU 保存一层的部分权重，在运算时，输入张量在每一张 GPU 上分别计算，然后通过 GPU 之间的通信聚合起来。\n",
    "\n",
    "相较于朴素的流水线并行，张量并行每一次计算均有多张 GPU 参与。\n",
    "\n",
    "相较于朴素的数据并行，张量并行每一张 GPU 保存部分模型权重。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.实验流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 环境配置\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/NVIDIA/Megatron-LM.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch einops pybind11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. 包导入以及通信组设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"./Megatron-LM\")\n",
    "\n",
    "\n",
    "from megatron.core import parallel_state\n",
    "from megatron.core import dist_checkpointing\n",
    "from megatron.core.pipeline_parallel.schedules import get_forward_backward_func\n",
    "from megatron.core.tensor_parallel.random import model_parallel_cuda_manual_seed\n",
    "from megatron.core.transformer.transformer_config import TransformerConfig\n",
    "from megatron.core.models.gpt.gpt_model import GPTModel\n",
    "from megatron.core.models.gpt.gpt_layer_specs import get_gpt_layer_local_spec\n",
    "from megatron.core.datasets.utils import compile_helpers \n",
    "from megatron.core.datasets.blended_megatron_dataset_builder import BlendedMegatronDatasetBuilder\n",
    "from megatron.core.datasets.gpt_dataset import GPTDatasetConfig, MockGPTDataset\n",
    "from megatron.training.tokenizer.tokenizer import _NullTokenizer\n",
    "\n",
    "\n",
    "_SEQUENCE_LENGTH = 64\n",
    "\n",
    "\n",
    "def initialize_distributed(tensor_model_parallel_size=1, pipeline_model_parallel_size=1):\n",
    "    parallel_state.destroy_model_parallel()\n",
    "\n",
    "    # Torch setup for distributed training\n",
    "    rank = int(os.environ['LOCAL_RANK'])\n",
    "    world_size = torch.cuda.device_count()\n",
    "    torch.cuda.set_device(rank)\n",
    "    torch.distributed.init_process_group(world_size=world_size, rank=rank)\n",
    "\n",
    "    # Megatron core distributed training initialization\n",
    "    parallel_state.initialize_model_parallel(tensor_model_parallel_size, pipeline_model_parallel_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. transformer模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_provider():\n",
    "    \"\"\"Build the model.\"\"\"\n",
    "\n",
    "    transformer_config = TransformerConfig(\n",
    "        num_layers=10, \n",
    "        hidden_size=512, \n",
    "        num_attention_heads=4, \n",
    "        use_cpu_initialization=True, \n",
    "        pipeline_dtype=torch.float32,\n",
    "    )\n",
    "\n",
    "    gpt_model = GPTModel(\n",
    "        config=transformer_config, \n",
    "        transformer_layer_spec=get_gpt_layer_local_spec(), \n",
    "        vocab_size=100, \n",
    "        max_sequence_length=_SEQUENCE_LENGTH,\n",
    "    )\n",
    "\n",
    "    return gpt_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.数据集定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data_iterator():\n",
    "    if torch.distributed.is_available() and torch.distributed.is_initialized():\n",
    "        if torch.distributed.get_rank() == 0:\n",
    "            compile_helpers()\n",
    "        torch.distributed.barrier()\n",
    "    else:\n",
    "        compile_helpers()\n",
    "\n",
    "    config = GPTDatasetConfig(\n",
    "        random_seed=0,\n",
    "        sequence_length=_SEQUENCE_LENGTH,\n",
    "        reset_position_ids=False,\n",
    "        reset_attention_mask=False,\n",
    "        eod_mask_loss=False,\n",
    "        tokenizer=_NullTokenizer(vocab_size=_SEQUENCE_LENGTH),\n",
    "    )\n",
    "\n",
    "    datasets = BlendedMegatronDatasetBuilder(\n",
    "        MockGPTDataset, [1000, None, None], lambda: True, config\n",
    "    ).build()\n",
    "\n",
    "    train_dataloader = DataLoader(datasets[0], batch_size=128, shuffle=True)\n",
    "\n",
    "    train_iterator = iter(train_dataloader)\n",
    "\n",
    "    return train_iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.训练核心代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_step_func(data_iterator, model):\n",
    "\n",
    "    def loss_func(loss_mask: torch.Tensor, output_tensor: torch.Tensor):\n",
    "\n",
    "        losses = output_tensor.float()\n",
    "        loss_mask = loss_mask.view(-1).float()\n",
    "        loss = torch.sum(losses.view(-1) * loss_mask) / loss_mask.sum()\n",
    "        # If you have data parallel reduce loss across data parallel groups.\n",
    "        # If pipeline parallel, loss computation is done only in last stage.\n",
    "\n",
    "        return loss, {'lm loss': loss}\n",
    "\n",
    "    data = next(data_iterator)\n",
    "    tokens = data['tokens'].to(device)\n",
    "    attention_mask = data['attention_mask'].to(device)\n",
    "    position_ids = data['position_ids'].to(device)\n",
    "    labels = data['labels'].to(device)\n",
    "    loss_mask = data['loss_mask'].to(device)\n",
    "\n",
    "    output_tensor = model(tokens, position_ids, attention_mask,\n",
    "                          labels=labels)\n",
    "\n",
    "    return output_tensor, partial(loss_func, loss_mask)\n",
    "\n",
    "def save_distributed_checkpoint(checkpoint_path, gpt_model):\n",
    "    sharded_state_dict = gpt_model.sharded_state_dict(prefix='')\n",
    "    dist_checkpointing.save(sharded_state_dict=sharded_state_dict, checkpoint_dir=checkpoint_path)\n",
    "\n",
    "def load_distributed_checkpoint(checkpoint_path, gpt_model):\n",
    "    sharded_state_dict=gpt_model.sharded_state_dict(prefix='')\n",
    "    checkpoint = dist_checkpointing.load(sharded_state_dict=sharded_state_dict, checkpoint_dir=checkpoint_path)\n",
    "    gpt_model.load_state_dict(checkpoint)\n",
    "    return gpt_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd\n",
    "!CUDA_VISIBLE_DEVICES=6,7 torchrun --nproc-per-node 2 train.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2.5.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
