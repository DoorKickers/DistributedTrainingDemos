{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**因 Jupter 对多进程支持不完善，因此分布式实验的代码运行均需要通过脚本执行！！！！！！**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.背景介绍\n",
    "\n",
    "本实验主要研究 PyTorch 分布式训练框架下的多机多卡数据/流水线混合并行训练方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.实验目的\n",
    "实现基于 torch.distributed.pipelining 和 torch.nn.parallel.DistributedDataParallel 的并行训练，并理解其工作原理。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.硬件要求\n",
    "\n",
    "两台服务器，各两张 GPU（4090、V100、A100等）。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.技术原理\n",
    "\n",
    "### 混合并行\n",
    "\n",
    "在朴素的数据并行中，每个 GPU 都要保存一份完整的模型权重。若单张显卡显存不足，便可在数据并行组内，开启流水线并行，从而降低单张显卡的显存需求。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.实验流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 环境配置\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. transformer模型定义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入所需要的包，并定义了一个基本的 transformer 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.distributed as dist\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed.device_mesh import init_device_mesh\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import DistributedSampler\n",
    "from torch.distributed import ReduceOp\n",
    "import math\n",
    "import yaml\n",
    "import argparse\n",
    "\n",
    "from torch.distributed.pipelining import pipeline, SplitPoint, build_stage\n",
    "from torch.distributed.pipelining import ScheduleGPipe\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, : x.size(1)]\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.out_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        q = (\n",
    "            self.q_linear(x)\n",
    "            .view(batch_size, -1, self.num_heads, self.d_k)\n",
    "            .transpose(1, 2)\n",
    "        )\n",
    "        k = (\n",
    "            self.k_linear(x)\n",
    "            .view(batch_size, -1, self.num_heads, self.d_k)\n",
    "            .transpose(1, 2)\n",
    "        )\n",
    "        v = (\n",
    "            self.v_linear(x)\n",
    "            .view(batch_size, -1, self.num_heads, self.d_k)\n",
    "            .transpose(1, 2)\n",
    "        )\n",
    "\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float(\"-1e9\"))\n",
    "\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attn_weights, v)\n",
    "\n",
    "        output = (\n",
    "            output.transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .view(batch_size, -1, self.num_heads * self.d_k)\n",
    "        )\n",
    "        return self.out_linear(output)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(F.relu(self.fc1(x)))\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = MultiHeadSelfAttention(d_model, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.ffn = FeedForward(d_model, d_ff)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_out = self.attention(x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_out))\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.norm2(x + self.dropout(ffn_out))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        d_model=512,\n",
    "        num_heads=8,\n",
    "        d_ff=2048,\n",
    "        num_layers=6,\n",
    "        max_len=5000,\n",
    "    ):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [TransformerBlock(d_model, num_heads, d_ff) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.out_linear = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return self.out_linear(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.数据集定义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该类定义了一个模拟数据集，其中第 i 条数据类似 [i, i, i, i, i, i, i]\n",
    "\n",
    "在训练的过程中，我们希望模型能够捕捉到这条性质"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLPDataset(Dataset):\n",
    "    def __init__(self, size, length):\n",
    "        self.data = []\n",
    "        for i in range(size):\n",
    "            self.data.append(torch.full((length, ), i))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "dataset = NLPDataset(12, 10)\n",
    "for data in dataset:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3训练核心代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_yaml_config(file_path):\n",
    "    # 解析 YAML 配置文件，读取模型、数据集、训练、并行配置参数\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "\n",
    "    # 模型结构参数\n",
    "    model_args = {\n",
    "        \"vocab_size\": config.get(\"vocab_size\"),\n",
    "        \"max_seq_length\": config.get(\"max_seq_length\"),\n",
    "        \"hidden_size\": config.get(\"hidden_size\"),\n",
    "        \"feedforward_size\": config.get(\"feedforward_size\"),\n",
    "        \"num_heads\": config.get(\"num_heads\"),\n",
    "        \"num_layers\": config.get(\"num_layers\"),\n",
    "    }\n",
    "\n",
    "    # 数据集参数\n",
    "    dataset_args = {\n",
    "        \"dataset_size\": config.get(\"dataset_size\"),\n",
    "        \"data_length\": config.get(\"data_length\"),\n",
    "    }\n",
    "\n",
    "    # 训练参数\n",
    "    training_args = {\n",
    "        \"train_epochs\": config.get(\"train_epochs\"),\n",
    "        \"micro_batch_size\": config.get(\"micro_batch_size\"),\n",
    "        \"micro_num\": config.get(\"micro_num\"),\n",
    "        \"learning_rate\": config.get(\"learning_rate\"),\n",
    "        \"device_type\": config.get(\"device_type\"),\n",
    "    }\n",
    "\n",
    "    # 由微批数和微批大小计算总 batch size\n",
    "    training_args[\"batch_size\"] = (\n",
    "        training_args[\"micro_num\"] * training_args[\"micro_batch_size\"]\n",
    "    )\n",
    "\n",
    "    # 并行参数\n",
    "    parallel_args = {\n",
    "        \"pipeline_parallel_size\": config.get(\"pipeline_parallel_size\"),\n",
    "        \"data_parallel_size\": config.get(\"data_parallel_size\"),\n",
    "    }\n",
    "\n",
    "    return model_args, dataset_args, training_args, parallel_args\n",
    "\n",
    "\n",
    "def train(rank, world_size, config_file):\n",
    "    model_args, dataset_args, training_args, parallel_args = parse_yaml_config(config_file)\n",
    "\n",
    "    # 定义 loss 函数\n",
    "    def compute_loss(output, target):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        loss = criterion(output.view(-1, model_args[\"vocab_size\"]), target.view(-1))\n",
    "        return loss\n",
    "\n",
    "    # 创建一个微批的输入占位符（用于推理管道构建）\n",
    "    x = torch.zeros(\n",
    "        (training_args[\"micro_batch_size\"], dataset_args[\"data_length\"] - 1),\n",
    "        dtype=torch.long,\n",
    "    )\n",
    "\n",
    "    # 构建流水线切割点，按层划分阶段\n",
    "    split_spec = {}\n",
    "    for i in range(parallel_args[\"pipeline_parallel_size\"] - 1):\n",
    "        # 按等间隔方式划分 Transformer 层的切割点\n",
    "        layers_id = (\n",
    "            (model_args[\"num_layers\"] - 1)\n",
    "            // parallel_args[\"pipeline_parallel_size\"]\n",
    "            * (i + 1)\n",
    "        )\n",
    "        split_spec[f\"layers.{layers_id}\"] = SplitPoint.END\n",
    "\n",
    "    # 构建流水线模型，进行 pipeline 切分\n",
    "    pipe = pipeline(\n",
    "        module=Transformer(\n",
    "            vocab_size=model_args[\"vocab_size\"],\n",
    "            d_model=model_args[\"hidden_size\"],\n",
    "            num_heads=model_args[\"num_heads\"],\n",
    "            d_ff=model_args[\"feedforward_size\"],\n",
    "            num_layers=model_args[\"num_layers\"],\n",
    "            max_len=model_args[\"max_seq_length\"],\n",
    "        ),\n",
    "        mb_args=(x,),\n",
    "        split_spec=split_spec,\n",
    "    )\n",
    "\n",
    "    # 初始化 2D 并行设备网格（data 并行 x pipeline 并行）\n",
    "    # 例如，4 张 GPU 进行训练，编号为 0 1 2 3，一种 2D 并行方法为：GPU 0 1 为一个DP组，两张卡负责存储完整的模型权重，其中 0 号卡负责前半部分模型，1 号卡后半部分模型。 2 3 号卡同理。\n",
    "    # 这样 4 张 GPU 便可以存储两个完整的模型权重，每张 GPU 需要模型权重一半的显存。在节省显存的同时，因为同时处理两个 batch 的数据，也提高了训练的速度。\n",
    "    # device_mesh 即定义了这种二维设备关系，例如哪两个卡属于同一个 DP 组。\n",
    "    mesh_2d = init_device_mesh(\n",
    "        training_args[\"device_type\"],\n",
    "        mesh_shape=(\n",
    "            parallel_args[\"data_parallel_size\"],\n",
    "            parallel_args[\"pipeline_parallel_size\"],\n",
    "        ),\n",
    "        mesh_dim_names=(\"dp\", \"pp\"),\n",
    "    )\n",
    "\n",
    "    # 获取当前进程在 pipeline 和 data 并行中的 rank\n",
    "    pp_group = mesh_2d.get_group(\"pp\")\n",
    "    dp_group = mesh_2d.get_group(\"dp\")\n",
    "\n",
    "    # pp rank 指的是该进程负责保存模型前半部分还是后半部分\n",
    "    # dp rank 指的是该进程属于哪个 dp 组，相同 dp 组内的卡共同保存模型的权重\n",
    "    pp_rank = dist.get_rank(pp_group)\n",
    "    dp_rank = dist.get_rank(dp_group)\n",
    "\n",
    "    # 设置当前 rank 使用的设备\n",
    "    device = f\"cuda:{rank % 2}\" if training_args[\"device_type\"] == \"gpu\" else \"cpu\"\n",
    "\n",
    "    # 获取该 stage 的模型模块，并移动到设备\n",
    "    stage_mod = pipe.get_stage_module(pp_rank).to(device)\n",
    "    print(\"rank, stage_mod\", rank, stage_mod)\n",
    "\n",
    "    # 使用 DDP 包装当前模块，支持数据并行\n",
    "    dp_mod = DDP(\n",
    "        stage_mod,\n",
    "        device_ids=[rank] if not device == \"cpu\" else None,\n",
    "        process_group=dp_group,\n",
    "    )\n",
    "\n",
    "    # 构造优化器\n",
    "    optimizer = optim.SGD(dp_mod.parameters(), lr=training_args[\"learning_rate\"])\n",
    "\n",
    "    # 构建流水线执行阶段\n",
    "    info = pipe.info()\n",
    "    stage = build_stage(dp_mod, pp_rank, info, device, pp_group)\n",
    "\n",
    "    # 构建流水线调度器（GPipe），支持前后向分段流水并行\n",
    "    schedule = ScheduleGPipe(stage, training_args[\"micro_num\"], compute_loss)\n",
    "\n",
    "    # 构建数据集和分布式采样器\n",
    "    dataset = NLPDataset(\n",
    "        size=dataset_args[\"dataset_size\"], length=dataset_args[\"data_length\"]\n",
    "    )\n",
    "    sampler = DistributedSampler(\n",
    "        dataset, num_replicas=parallel_args[\"data_parallel_size\"], rank=dp_rank\n",
    "    )\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=training_args[\"batch_size\"], sampler=sampler\n",
    "    )\n",
    "\n",
    "    # 训练主循环\n",
    "    # 在训练的主循环里，不仅用到了 DP ，也用到了 PP 。\n",
    "    # 对于 DP，我们需要维护一个 DistributedSampler，对于 PP 我们有Pipeline Schedular。\n",
    "    # DDP 将模型 warp 以后，在反向传播以后会自动同步梯度，而反向传播的具体过程，又由 schedular 提供，因为 torch 的工程实现，我们组合两种并行方法变得较为简单。\n",
    "    for epoch in range(training_args[\"train_epochs\"]):\n",
    "        for batch, data in enumerate(dataloader):\n",
    "            label = data[:, 1:].to(device)\n",
    "            x = data[:, :-1].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if pp_rank == 0:\n",
    "                # 第一个阶段只进行前向传播\n",
    "                schedule.step(x)\n",
    "            else:\n",
    "                # 最后一个阶段进行前向 + 反向传播，并收集 loss\n",
    "                losses = []\n",
    "                output = schedule.step(target=label, losses=losses)\n",
    "                loss = torch.stack(losses).mean()\n",
    "\n",
    "                # 对 loss 进行 data-parallel 级别的 all-reduce\n",
    "                dist.all_reduce(loss, op=ReduceOp.SUM, group=dp_group)\n",
    "                if dp_rank == 0:\n",
    "                    print(\n",
    "                        f\"Epoch {epoch}, Batch {batch}, Loss: {loss / parallel_args['data_parallel_size']}\"\n",
    "                    )\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "    # 销毁进程组，释放资源\n",
    "    dist.destroy_process_group()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4启动训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 修改 run.sh 中服务器地址\n",
    "- 第 i 台机器对应的启动命令为 bash run.sh i\n",
    "- 本次实验需要用到两台机器，每台机器有两张 GPU，那么需要首先获得 master 地址，然后修改脚本，填写 master 地址，在节点 1 上执行 bash run.sh 0，在节点 2 上执行 bash run.sh 1\n",
    "\n",
    "node_rank=$1\n",
    "\n",
    "torchrun --nnodes 2 --node-rank $node_rank --nproc_per_node=2 --master-addr 1.1.1.1 --master_port=11111 ddp_pp_train.py --config config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash run.sh 0/1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
