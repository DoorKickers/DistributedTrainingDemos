{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**因 Jupter 对多进程支持不完善，因此分布式实验的代码运行均需要通过脚本执行！！！！！！**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.背景介绍\n",
    "\n",
    "本实验主要研究 PyTorch 分布式训练框架下的单机多卡（Single Machine Multi-GPU）流水线并行训练方法，重点探索 torch.distributed.pipelining 的使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.实验目的\n",
    "实现基于 torch.distributed.pipelining 的流水线并行训练，并理解其工作原理。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.硬件要求\n",
    "\n",
    "2张 GPU（4090、V100、A100等）。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.技术原理\n",
    "\n",
    "### 流水线并行（Pipeline Parallelism）\n",
    "\n",
    "流水线并行指将模型的不同部分放在不同的GPU上，从而降低单个GPU显存使用量。\n",
    "\n",
    "例如，如果有模型为以下结构: [embedding -> pos_encoding -> layer.0 -> layer.1 -> layer.2 -> layer.3 -> out_linear]\n",
    "\n",
    "我们可以将 [embedding -> pos_encoding -> layer.0 -> layer.1] 这部分模型放到 GPU 0 上，将其余部分 ( [layer.2 -> layer.3 -> out_linear] ) 放到 GPU 1 上。\n",
    "\n",
    "在前向传播的时候，输入数据经过 GPU 0，计算得到 layer.1 的输出，然后此时 GPU 0 与 GPU 1 之间进行一次张量通信，将 layer.1 的输出发送给 GPU 1。 随后 GPU 1 完成其余部分模型的计算\n",
    "\n",
    "同理，在反向传播的时候，不同 GPU 之间 也会进行梯度的通信。\n",
    "\n",
    "### micro-batch 优化\n",
    "\n",
    "朴素的流水线并行效率不高，是因为同一时间只有一张 GPU 在进行计算。为了提高 GPU 的利用效率，micro-batch 优化技术将一个 input batch 分割成多个 micro-batch， 从而提高数据在多块 GPU 上的流转效率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.实验流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 环境配置\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. transformer模型定义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入所需要的包，并定义了一个基本的 transformer 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.distributed as dist\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import DistributedSampler\n",
    "import math\n",
    "\n",
    "\n",
    "from torch.distributed.pipelining import pipeline, SplitPoint\n",
    "from torch.distributed.pipelining import ScheduleGPipe\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, : x.size(1)]\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.out_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        q = (\n",
    "            self.q_linear(x)\n",
    "            .view(batch_size, -1, self.num_heads, self.d_k)\n",
    "            .transpose(1, 2)\n",
    "        )\n",
    "        k = (\n",
    "            self.k_linear(x)\n",
    "            .view(batch_size, -1, self.num_heads, self.d_k)\n",
    "            .transpose(1, 2)\n",
    "        )\n",
    "        v = (\n",
    "            self.v_linear(x)\n",
    "            .view(batch_size, -1, self.num_heads, self.d_k)\n",
    "            .transpose(1, 2)\n",
    "        )\n",
    "\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float(\"-1e9\"))\n",
    "\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attn_weights, v)\n",
    "\n",
    "        output = (\n",
    "            output.transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .view(batch_size, -1, self.num_heads * self.d_k)\n",
    "        )\n",
    "        return self.out_linear(output)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(F.relu(self.fc1(x)))\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = MultiHeadSelfAttention(d_model, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.ffn = FeedForward(d_model, d_ff)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_out = self.attention(x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_out))\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.norm2(x + self.dropout(ffn_out))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        d_model=512,\n",
    "        num_heads=8,\n",
    "        d_ff=2048,\n",
    "        num_layers=6,\n",
    "        max_len=5000,\n",
    "    ):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [TransformerBlock(d_model, num_heads, d_ff) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.out_linear = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return self.out_linear(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.数据集定义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该类定义了一个模拟数据集，其中第 i 条数据类似 [i, i, i, i, i, i, i]\n",
    "\n",
    "在训练的过程中，我们希望模型能够捕捉到这条性质"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLPDataset(Dataset):\n",
    "    def __init__(self, size, length):\n",
    "        self.data = []\n",
    "        for i in range(size):\n",
    "            self.data.append(torch.full((length, ), i))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "dataset = NLPDataset(12, 10)\n",
    "for data in dataset:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3训练核心代码实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该段训练代码实现了主要的训练逻辑，包括数据集的使用，模型的实例化，pipeline方法和具体的训练步骤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(rank, world_size):\n",
    "    # Transformer模型的基本参数\n",
    "    VOCAB_SIZE = 100        # 词表大小\n",
    "    D_MODEL = 12            # 模型的嵌入维度（hidden size）\n",
    "    NUM_HEADS = 4           # 多头注意力的头数\n",
    "    D_FF = 24               # 前馈网络维度\n",
    "    NUM_LAYERS = 2          # Transformer 层数\n",
    "    MAX_LEN = 100           # 序列最大长度\n",
    "\n",
    "    # 数据集相关参数\n",
    "    DATASET_SIZE = 12       # 数据集中样本数量\n",
    "    DATASET_LENGTH = 10     # 每个样本的序列长度\n",
    "    BATCH_SIZE = 4          # 每个 batch 的样本数\n",
    "\n",
    "    NUM_MICROBATCHES = 2    # 用于 GPipe 分割的 microbatch 数\n",
    "\n",
    "    # 定义损失函数的封装（方便被 GPipe 调用）\n",
    "    def compute_loss(output, target):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        loss = criterion(output.view(-1, VOCAB_SIZE), target.view(-1))\n",
    "        return loss\n",
    "\n",
    "    # 构造 NLP 数据集和 DataLoader\n",
    "    dataset = NLPDataset(size=DATASET_SIZE, length=DATASET_LENGTH)\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "    # 构造一个输入模板，用于 GPipe pipeline 初始化\n",
    "    x = torch.zeros((BATCH_SIZE // NUM_MICROBATCHES, DATASET_LENGTH - 1), dtype=torch.long)\n",
    "\n",
    "    # 构建带有 split_spec 的 GPipe pipeline，将模型划分为多个 stage\n",
    "    # 流水线并行的关键是怎么划分一个较大的模型，将模型的不同部分放到不同的 GPU 上。pytorch 的 pipeline 方法，提供了一个基于计算图和编译技术的划分方式，从而在处理不同模型的时候，用户不需要手动将模型切分出来，提高了使用的便利性。\n",
    "    pipe = pipeline(\n",
    "        module=Transformer(\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            d_model=D_MODEL,\n",
    "            num_heads=NUM_HEADS,\n",
    "            d_ff=D_FF,\n",
    "            num_layers=NUM_LAYERS,\n",
    "            max_len=MAX_LEN,\n",
    "        ),\n",
    "        mb_args=(x,),  # microbatch 的输入形状，用于模型切分\n",
    "        split_spec={\n",
    "            \"layers.1\": SplitPoint.BEGINNING,  # 从 Transformer 第2层开始划分模型\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # 通过环境变量获取当前进程的rank和world size\n",
    "    rank = int(os.environ[\"RANK\"])\n",
    "    world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "\n",
    "    # 初始化 PyTorch 分布式通信\n",
    "    dist.init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n",
    "\n",
    "    # 指定当前 rank 使用的 GPU 设备\n",
    "    device = f\"cuda:{rank}\"\n",
    "\n",
    "    # 获取当前 rank 的模型 stage，用于定义 optimizer\n",
    "    stage_mod = pipe.get_stage_module(rank)\n",
    "    optimizer = optim.SGD(stage_mod.parameters(), lr=0.01)\n",
    "\n",
    "    # 构建模型 stage，并设置 device 和通信信息\n",
    "    stage = pipe.build_stage(rank, device, None)\n",
    "\n",
    "    # 创建 GPipe 的训练调度器（ScheduleGPipe）\n",
    "    # GPipe 调度器具体处理了 microbatch 优化的相关逻辑，负责切分，发送，stage 之间的发送，以及反向传播的过程。\n",
    "    # 不同于数据并行，流水线并行的调度流程更为复杂，形式上不能和 DDP 一样，warp 成一个带有 forward 方法的 model。例如只有最后一个 stage 需要计算 loss，而且反向传播的正向传播的过程中，不同 stage 都需要进行通信。\n",
    "    # 而朴素的数据并行在前向传播和反向传播的过程，不同进程组是完全独立的，进程编号不会影响结构，因此实现逻辑上简单一些。\n",
    "    # 用户在使用的时候，需要将损失函数的计算逻辑传递给 Schedular。\n",
    "    schedule = ScheduleGPipe(stage, NUM_MICROBATCHES, compute_loss)\n",
    "\n",
    "    # 正式开始训练过程\n",
    "    for epoch in range(200):\n",
    "        for batch, data in enumerate(dataloader):\n",
    "            # 将输入数据切分为 input 和 target\n",
    "            label = data[:, 1:].to(device)  # 目标序列（标签）\n",
    "            x = data[:, :-1].to(device)     # 输入序列（模型输入）\n",
    "\n",
    "            optimizer.zero_grad()           # 梯度清零\n",
    "\n",
    "            # 对于 rank 0：只负责前向传输，不计算 loss\n",
    "            if rank == 0:\n",
    "                schedule.step(x)\n",
    "            else:\n",
    "                # 其他 stage（最后一段）负责 loss 计算和反向传播\n",
    "                losses = []\n",
    "                output = schedule.step(target=label, losses=losses)\n",
    "                print(\n",
    "                    f\"Epoch {epoch}, Batch {batch}, Loss: {torch.stack(losses).mean()}\"\n",
    "                )\n",
    "\n",
    "            # 优化参数\n",
    "            optimizer.step()\n",
    "\n",
    "    # 销毁进程组，结束训练\n",
    "    dist.destroy_process_group()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4启动训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash run.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
